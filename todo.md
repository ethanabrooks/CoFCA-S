# `goal_space`-restricted goals
* Double check whether small initialization bug is affecting things.
* try a non-retain_graph version
* try updating gan on batch_size=1
* refine hyperparams for exp and baseline
* try to eliminate so-called "regularizer" in GAN
* adapt PPO to multi-goal (where constraint applies to multiple tasks)
* sort out Thomas's advice on PyTorch forum
* try summing vs. norm

# unrestricted goals
* Use reward mean instead of value function for unsupervised loss
